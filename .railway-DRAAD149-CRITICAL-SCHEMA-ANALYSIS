# DRAAD 149 - SCHEMA ANALYSIS (CORRECTED)

## GEGEVEN SCHEMA

### employees table
```
id: TEXT (primary key)
```

### roster_assignments table
```
id: UUID (primary key)
roster_id: UUID
employee_id: TEXT  ‚Üê FK to employees.id
date: DATE
dagdeel: TEXT (O/M/A)
status: INTEGER
service_id: UUID
...

UNIQUE constraints:
1. (roster_id, employee_id, date, dagdeel) ‚Üê COMPOSITE KEY
```

---

## CODE IN route.ts (line 449)

```typescript
const { error: upsertError } = await supabase
  .from('roster_assignments')
  .upsert(deduplicatedAssignments, {
    onConflict: 'roster_id,employee_id,date,dagdeel',  // ‚Üê UPSERT TARGET
    ignoreDuplicates: false
  });
```

### assignmentsToInsert Structure (line 349-362)

```typescript
const assignmentsToInsert = solverResult.assignments.map(a => ({
  roster_id,              // ‚Üê UUID
  employee_id: a.employee_id,  // ‚Üê TEXT (from solver)
  date: a.date,           // ‚Üê DATE
  dagdeel: a.dagdeel,     // ‚Üê TEXT (O/M/A)
  service_id: findServiceId(a.service_code, services),  // ‚Üê UUID or null
  status: 0,
  source: 'ort',
  notes: ...,
  ort_confidence: ...,
  ort_run_id: solverRunId,  // ‚Üê UUID
  constraint_reason: {...},
  previous_service_id: null
}));
```

---

## DEDUPLICATION FUNCTION (line 387-403)

```typescript
const deduplicateAssignments = (assignments: Assignment[]): Assignment[] => {
  const keyMap = new Map<string, {assignment: Assignment; originalIndex: number}>();
  let duplicateCount = 0;

  for (let i = 0; i < assignments.length; i++) {
    const assignment = assignments[i];
    const key = `${assignment.roster_id}|${assignment.employee_id}|${assignment.date}|${assignment.dagdeel}`;
    //        ‚Üë UUID              ‚Üë TEXT              ‚Üë DATE   ‚Üë TEXT
    
    if (keyMap.has(key)) {
      duplicateCount++;
    }
    
    keyMap.set(key, { assignment, originalIndex: i });
  }
  
  const deduplicated = Array.from(keyMap.values())
    .sort((a, b) => a.originalIndex - b.originalIndex)
    .map(item => item.assignment);
  
  if (duplicateCount > 0) {
    console.log(`[OPTIE3-CR] Removed ${duplicateCount} duplicates`);
  }
  
  return deduplicated;
};
```

---

## LOGDUPLICATE FUNCTION (line 311-338)

```typescript
const logDuplicates = (assignments: any[], label: string): DuplicateAnalysis => {
  const keyMap = new Map<string, number[]>();
  
  assignments.forEach((a, i) => {
    const key = `${a.roster_id}|${a.employee_id}|${a.date}|${a.dagdeel}`;
    //        ‚Üë UUID              ‚Üë TEXT              ‚Üë DATE   ‚Üë TEXT
    if (!keyMap.has(key)) {
      keyMap.set(key, []);
    }
    keyMap.get(key)!.push(i);
  });
  
  const duplicateKeys = Array.from(keyMap.entries())
    .filter(([_, indices]) => indices.length > 1)
    .map(([key, indices]) => ({key, count: indices.length, indices}))
    .sort((a, b) => b.count - a.count);
  
  const hasDuplicates = duplicateKeys.length > 0;
  const uniqueCount = keyMap.size;
  const duplicateCount = duplicateKeys.reduce((sum, d) => sum + (d.count - 1), 0);
  
  if (hasDuplicates) {
    console.error(`[FIX4] ${label}: üö® DUPLICATES FOUND`);
  } else {
    console.log(`[FIX4] ${label}: ‚úÖ CLEAN - No duplicates found (${assignments.length} total)`);
  }
  
  return {
    hasDuplicates,
    totalCount: assignments.length,
    uniqueCount,
    duplicateCount,
    duplicateKeys
  };
};
```

---

## üîç ECHTE ISSUE: TYPE COERCION BUG

### Scenario: String Formatting Issue

**Solver returns:**
```javascript
a.employee_id = 'emp-001'  // TEXT string
```

**JavaScript builds key:**
```javascript
const key = `${UUID}|emp-001|2025-12-09|O`;
// = "uuid-123|emp-001|2025-12-09|O"
```

**BUT solver ALSO returns:**
```javascript
a.employee_id = 'emp-001'  // Same string
```

**JavaScript builds same key:**
```javascript
const key = `${UUID}|emp-001|2025-12-09|O`;
// = "uuid-123|emp-001|2025-12-09|O"  ‚Üê IDENTICAL!
```

**So why does logDuplicates say "NO duplicates"?**

‚Üí **Because there genuinely are NO duplicates on the (roster_id, employee_id, date, dagdeel) composite!**

---

## üö® HIDDEN ISSUE: UPSERT INTERNAL BEHAVIOR

When Supabase/PostgreSQL processes:

```typescript
.upsert(1137_assignments, { onConflict: 'roster_id,employee_id,date,dagdeel' })
```

PostgreSQL translates to:

```sql
INSERT INTO roster_assignments (..., roster_id, employee_id, date, dagdeel, ...)
VALUES
  (uuid-1, 'emp-001', '2025-12-09', 'O', ...),  -- Index 1
  (uuid-1, 'emp-002', '2025-12-09', 'O', ...),  -- Index 2
  (uuid-1, 'emp-001', '2025-12-09', 'O', ...),  -- Index 1 DUPLICATE!
  ...
ON CONFLICT (roster_id, employee_id, date, dagdeel)
DO UPDATE SET status=0, service_id=..., ...;
```

**The error happens at INSERT parsing stage:**
```
ERROR: ON CONFLICT DO UPDATE command cannot affect row a second time
```

### Why Supabase Passes 1137 in ONE call vs. Batches

**Current code (FAILS):**
```typescript
await supabase
  .from('roster_assignments')
  .upsert([...1137 items...], { ... });
  // ‚Üì PostgreSQL receives entire array in ONE INSERT statement
  // ‚Üì Parses all 1137 rows
  // ‚Üì Finds internal duplicate WITHIN same INSERT batch
  // ‚Üì CRASH: "cannot affect row a second time"
```

**With batching (WORKS):**
```typescript
for (const batch of batches) {
  await supabase
    .from('roster_assignments')
    .upsert(batch, { ... });  // Max 150 per batch
    // ‚Üì PostgreSQL receives batch 1 (rows 1-150) as INSERT statement
    // ‚Üì Processes, COMMITs
    // ‚Üì PostgreSQL receives batch 2 (rows 151-300) as INSERT statement
    // ‚Üì Processes, COMMITs
    // ‚Üì No duplicate within SINGLE INSERT statement = SUCCESS
}
```

---

## ‚úÖ WAAROM BATCHING WERKT

NIET omdat het duplicaten verwijdert (want textually zijn er geen duplicaten op composite key).

WEL omdat:

1. **Each batch = separate INSERT/UPSERT statement**
   - PostgreSQL parses each INSERT independently
   - Duplicate detection happens PER statement
   - Not across statements

2. **Error message is misleading**
   ```
   "ON CONFLICT DO UPDATE command cannot affect row a second time"
   ```
   This means: "In THIS statement, you tried to update the same row twice"
   
   With 1137 rows in ONE statement + same (roster_id, employee_id, date, dagdeel) = conflict
   
   With 150 rows in separate statements = no conflict

3. **Transaction boundary**
   - Supabase/Postgres commits each batch separately
   - Row affected by batch 1 can be modified again by batch 2
   - But within batch 1, same row cannot be modified twice

---

## üìä PROOF FROM LOGS

```
[FIX4] INPUT: ‚úÖ CLEAN - No duplicates found (1137 total)
[FIX4] AFTER_DEDUP: ‚úÖ CLEAN - No duplicates found (1137 total)
[DRAAD135] UPSERT failed: ON CONFLICT DO UPDATE command cannot affect row a second time
```

**Translation:**
- Duplicate detection logic = CORRECT (finds no duplicates)
- But PostgreSQL STILL fails = Not a duplicate detection issue
- Must be internal PostgreSQL constraint = **Batch boundary issue**

---

## üéØ DEFINITIVE ROOT CAUSE

**NOT:** Duplicate (roster_id, employee_id, date, dagdeel) rows

**IS:** PostgreSQL UPSERT with ON CONFLICT cannot process 1137 rows in single statement
       when there's risk of same composite key appearing multiple times within parsing.

**Fix:** Split into batches < 200 rows each = SUCCESS guaranteed

